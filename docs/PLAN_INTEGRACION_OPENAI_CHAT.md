# ü§ñ Plan de Integraci√≥n: Chat NEURA ‚Üî OpenAI API

**Fecha:** 9 Octubre 2025  
**Objetivo:** Conectar el chat del Cockpit con OpenAI para respuestas reales de IA

---

## üìä An√°lisis del Estado Actual

### ‚úÖ Lo que YA existe:

1. **Frontend (`EconeuraCockpit.tsx`)**
   - ‚úÖ UI de chat completa y funcional
   - ‚úÖ 40 agentes en 10 departamentos
   - ‚úÖ Sistema de mensajes (user/assistant)
   - ‚ö†Ô∏è **PROBLEMA:** `handleSendMessage()` est√° usando respuestas simuladas (setTimeout con mensaje est√°tico)

2. **Funci√≥n `invokeAgent()` (`apps/web/src/utils/invokeAgent.ts`)**
   - ‚úÖ L√≥gica para detectar agentes NEURA
   - ‚úÖ Llamada a `/api/chat` en producci√≥n
   - ‚úÖ Llamada a `/api/invoke/:agentId` en desarrollo
   - ‚úÖ Manejo de errores y respuestas

3. **API OpenAI (`apps/web/api/chat.js`)**
   - ‚úÖ Endpoint `/api/chat` funcional
   - ‚úÖ Integraci√≥n con OpenAI GPT-4
   - ‚úÖ System prompts personalizados por agente
   - ‚úÖ Configurado en Vercel

4. **Backend Node.js (`apps/api_node/server.js`)**
   - ‚úÖ Servidor en puerto 8080
   - ‚úÖ Rutas `/api/invoke/:agentId`
   - ‚úÖ Proxy a Make.com (opcional)
   - ‚úÖ 11 rutas configuradas

5. **Servicios FastAPI (`services/neuras/*/app.py`)**
   - ‚úÖ 11 microservicios Python
   - ‚úÖ Integraci√≥n con Azure OpenAI
   - ‚úÖ Endpoints `/v1/task`
   - ‚ö†Ô∏è Actualmente no usados por el frontend

---

## üî¥ Problema Principal

**El chat del Cockpit NO est√° llamando a `invokeAgent()`**

```typescript
// ‚ùå C√ìDIGO ACTUAL (EconeuraCockpit.tsx l√≠nea 201-214)
const handleSendMessage = useCallback(async () => {
  if (!inputValue.trim() || !selectedAgent) return;
  
  const userMessage: Message = { 
    id: `msg-${Date.now()}`, 
    role: "user", 
    content: inputValue.trim(), 
    timestamp: new Date() 
  };
  
  setMessages((prev) => [...prev, userMessage]);
  setInputValue("");
  
  // ‚ùå SIMULACI√ìN: Respuesta falsa despu√©s de 1 segundo
  setTimeout(() => {
    const assistantMessage: Message = { 
      id: `msg-${Date.now()}-response`, 
      role: "assistant", 
      content: `[${selectedAgent.name}] Procesando tu solicitud: "${userMessage.content}"...`, 
      timestamp: new Date(), 
      agentId: selectedAgent.id, 
      agentName: selectedAgent.name 
    };
    setMessages((prev) => [...prev, assistantMessage]);
  }, 1000);
}, [inputValue, selectedAgent]);
```

**Resultado:** El chat muestra respuestas simuladas, NO usa OpenAI.

---

## üéØ Soluci√≥n: Conectar Chat con OpenAI

### Paso 1: Importar `invokeAgent()` en el Cockpit

```typescript
// En EconeuraCockpit.tsx, l√≠nea 26 (despu√©s de los imports de lucide-react)
import invokeAgent from "./utils/invokeAgent";
```

### Paso 2: Reemplazar `handleSendMessage()` con llamada real

```typescript
// ‚úÖ NUEVO C√ìDIGO (llamada real a OpenAI)
const handleSendMessage = useCallback(async () => {
  if (!inputValue.trim() || !selectedAgent) return;
  
  // 1. Agregar mensaje del usuario
  const userMessage: Message = { 
    id: `msg-${Date.now()}`, 
    role: "user", 
    content: inputValue.trim(), 
    timestamp: new Date() 
  };
  
  setMessages((prev) => [...prev, userMessage]);
  const userInput = inputValue.trim();
  setInputValue(""); // Limpiar input inmediatamente
  
  // 2. Agregar mensaje "pensando..." temporal
  const thinkingId = `msg-${Date.now()}-thinking`;
  const thinkingMessage: Message = {
    id: thinkingId,
    role: "assistant",
    content: "üí≠ Pensando...",
    timestamp: new Date(),
    agentId: selectedAgent.id,
    agentName: selectedAgent.name
  };
  
  setMessages((prev) => [...prev, thinkingMessage]);
  
  try {
    // 3. Llamar a invokeAgent() con OpenAI
    const response = await invokeAgent(
      selectedAgent.id,
      'azure', // o 'local' para desarrollo
      { input: userInput }
    );
    
    // 4. Reemplazar mensaje "pensando..." con respuesta real
    setMessages((prev) => 
      prev.map((msg) => 
        msg.id === thinkingId
          ? {
              ...msg,
              content: response.ok 
                ? response.output 
                : `‚ùå Error: ${response.output}`,
              timestamp: new Date()
            }
          : msg
      )
    );
    
  } catch (error: any) {
    // 5. Manejar error
    setMessages((prev) => 
      prev.map((msg) => 
        msg.id === thinkingId
          ? {
              ...msg,
              content: `‚ùå Error de conexi√≥n: ${error.message}`,
              timestamp: new Date()
            }
          : msg
      )
    );
  }
}, [inputValue, selectedAgent]);
```

---

## üìù Archivos a Modificar

### 1. `apps/web/src/EconeuraCockpit.tsx`

**Cambios:**
- ‚úÖ L√≠nea 26: Agregar `import invokeAgent from "./utils/invokeAgent";`
- ‚úÖ L√≠neas 201-214: Reemplazar `handleSendMessage()` con c√≥digo real (ver arriba)

**Resultado esperado:**
- Chat muestra "üí≠ Pensando..." mientras espera respuesta
- Respuesta real de GPT-4 reemplaza el mensaje temporal
- Errores se muestran claramente al usuario

### 2. `apps/web/src/utils/invokeAgent.ts` (sin cambios)

**Estado actual:** ‚úÖ Ya est√° optimizado
- Detecta si es producci√≥n o desarrollo
- Usa `/api/chat` (OpenAI) en producci√≥n
- Usa `/api/invoke/:agentId` (backend Node.js) en desarrollo

### 3. `apps/web/api/chat.js` (sin cambios)

**Estado actual:** ‚úÖ Ya funcional
- Recibe: `{ agentId, message, context }`
- Llama a OpenAI GPT-4
- Retorna: `{ success, message, agentId, timestamp }`

---

## üß™ Plan de Testing

### Prueba 1: Desarrollo Local

```bash
# Terminal 1: Iniciar backend Node.js
cd apps/api_node
node server.js
# ‚úÖ Backend listening on http://127.0.0.1:8080

# Terminal 2: Iniciar frontend
cd apps/web
pnpm dev
# ‚úÖ Vite dev server on http://localhost:3000
```

**Probar:**
1. Abrir `http://localhost:3000`
2. Seleccionar departamento (ej: Executive)
3. Seleccionar agente (ej: CEO Vision)
4. Escribir: "¬øCu√°l es la visi√≥n estrat√©gica?"
5. **Verificar:** Deber√≠a mostrar respuesta del backend Node.js (modo simulaci√≥n o Make.com)

### Prueba 2: Producci√≥n (Vercel)

```bash
# Deploy a Vercel
vercel --prod
```

**Probar:**
1. Abrir URL de Vercel (ej: `econeura-punto.vercel.app`)
2. Seleccionar agente NEURA (CEO, CFO, CTO, etc.)
3. Escribir mensaje de prueba
4. **Verificar:** Respuesta real de OpenAI GPT-4

### Casos de Prueba

| # | Escenario | Input | Output Esperado |
|---|-----------|-------|-----------------|
| 1 | Agente CEO | "¬øCu√°l es nuestra misi√≥n?" | Respuesta ejecutiva de GPT-4 |
| 2 | Agente CFO | "Analiza estos n√∫meros: 100k revenue" | An√°lisis financiero de GPT-4 |
| 3 | Sin API key | Cualquier mensaje | Error claro: "OpenAI API key not configured" |
| 4 | Timeout | Mensaje muy complejo | Error: "Request timed out" |
| 5 | Agente no v√°lido | ID incorrecto | Error 404 o simulaci√≥n |

---

## üîß Variables de Entorno Necesarias

### Desarrollo (`.env.local`)

```bash
# Backend Node.js (opcional - para Make.com)
PORT=8080
MAKE_FORWARD=0  # 0 = simulaci√≥n, 1 = forwarding a Make.com
MAKE_TOKEN=

# OpenAI (no se usa en dev, solo en prod)
OPENAI_API_KEY=
```

### Producci√≥n (Vercel)

```bash
# ‚úÖ CR√çTICO: Configurar en Vercel Dashboard
OPENAI_API_KEY=sk-proj-tu-clave-real-aqui

# Opcional (si usas Azure OpenAI)
AZURE_OPENAI_API_ENDPOINT=https://econeura.openai.azure.com
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_API_VERSION=2024-02-01
AZURE_OPENAI_DEPLOYMENT=gpt-4
```

**C√≥mo configurar en Vercel:**
1. Ir a dashboard: https://vercel.com/econeura/econeura-punto
2. Settings ‚Üí Environment Variables
3. Agregar `OPENAI_API_KEY` con valor real
4. Redeploy: `vercel --prod`

---

## üìä Flujo de Datos Completo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Usuario escribe‚îÇ
‚îÇ  mensaje en UI  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ EconeuraCockpit.tsx     ‚îÇ
‚îÇ handleSendMessage()     ‚îÇ
‚îÇ - Agrega mensaje user   ‚îÇ
‚îÇ - Muestra "Pensando..." ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ invokeAgent()          ‚îÇ
‚îÇ - Detecta PROD vs DEV  ‚îÇ
‚îÇ - Elige endpoint       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ DEV ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                         ‚îÇ
         ‚îÇ                         ‚ñº
         ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ              ‚îÇ Backend Node.js       ‚îÇ
         ‚îÇ              ‚îÇ /api/invoke/:agentId  ‚îÇ
         ‚îÇ              ‚îÇ - Modo simulaci√≥n     ‚îÇ
         ‚îÇ              ‚îÇ - O proxy a Make.com  ‚îÇ
         ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                          ‚îÇ
         ‚îÇ                          ‚ñº
         ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ              ‚îÇ Make.com Webhooks     ‚îÇ
         ‚îÇ              ‚îÇ (opcional)            ‚îÇ
         ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                          ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îî‚îÄ PROD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                   ‚îÇ
                                   ‚ñº
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ /api/chat.js         ‚îÇ
                        ‚îÇ (Vercel Serverless)  ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
                                   ‚ñº
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ OpenAI GPT-4 API     ‚îÇ
                        ‚îÇ - System prompt      ‚îÇ
                        ‚îÇ - User message       ‚îÇ
                        ‚îÇ - Temperature 0.7    ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Respuesta AI           ‚îÇ
‚îÇ - output: "..."        ‚îÇ
‚îÇ - ok: true/false       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ UI actualiza mensaje   ‚îÇ
‚îÇ - Reemplaza "Pensando" ‚îÇ
‚îÇ - Muestra respuesta    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚ö° Optimizaciones Futuras

### 1. Streaming de Respuestas (GPT-4 Turbo)

Actualmente la respuesta llega toda de golpe. Podemos hacer streaming:

```typescript
// En /api/chat.js
const stream = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [...],
  stream: true  // ‚úÖ Activar streaming
});

// Enviar Server-Sent Events (SSE)
res.setHeader('Content-Type', 'text/event-stream');
for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || '';
  res.write(`data: ${JSON.stringify({ content })}\n\n`);
}
res.end();
```

**Beneficio:** Usuario ve el texto aparecer palabra por palabra (mejor UX)

### 2. Historial de Conversaci√≥n

Actualmente cada mensaje es independiente. Podemos enviar contexto:

```typescript
// En handleSendMessage()
const context = messages.slice(-5).map(msg => ({
  role: msg.role,
  content: msg.content
}));

const response = await invokeAgent(
  selectedAgent.id,
  'azure',
  { 
    input: userInput,
    context: context  // ‚úÖ √öltimos 5 mensajes
  }
);
```

**Beneficio:** GPT-4 recuerda el contexto de la conversaci√≥n

### 3. Indicadores de Typing (...)

```typescript
// En handleSendMessage(), antes de llamar invokeAgent()
const typingInterval = setInterval(() => {
  setMessages(prev => 
    prev.map(msg => 
      msg.id === thinkingId
        ? { ...msg, content: msg.content + '.' }
        : msg
    )
  );
}, 500);

// Despu√©s de recibir respuesta
clearInterval(typingInterval);
```

**Beneficio:** Feedback visual m√°s din√°mico

### 4. Rate Limiting

```typescript
// En /api/chat.js
const rateLimiter = new Map();

export default async function handler(req, res) {
  const userIp = req.headers['x-forwarded-for'] || req.socket.remoteAddress;
  const now = Date.now();
  const lastRequest = rateLimiter.get(userIp) || 0;
  
  if (now - lastRequest < 2000) {  // Max 1 req cada 2 segundos
    return res.status(429).json({ error: 'Too many requests' });
  }
  
  rateLimiter.set(userIp, now);
  // ... resto del c√≥digo
}
```

**Beneficio:** Protege contra abuso de API (costos OpenAI)

---

## üöÄ Implementaci√≥n Paso a Paso

### Fase 1: Cambio M√≠nimo (15 min)

1. ‚úÖ Agregar import en `EconeuraCockpit.tsx`
2. ‚úÖ Reemplazar `handleSendMessage()`
3. ‚úÖ Probar en desarrollo local
4. ‚úÖ Commit + push

### Fase 2: Testing (10 min)

1. ‚úÖ Probar 5 casos de prueba
2. ‚úÖ Verificar errores se muestran bien
3. ‚úÖ Verificar respuestas de GPT-4

### Fase 3: Deploy (5 min)

1. ‚úÖ `git push origin main`
2. ‚úÖ Vercel auto-deploy
3. ‚úÖ Probar en producci√≥n

**Tiempo total:** ~30 minutos

---

## üìà M√©tricas de √âxito

| M√©trica | Antes | Despu√©s | Meta |
|---------|-------|---------|------|
| Respuestas reales | 0% | 100% | ‚úÖ |
| Latencia promedio | N/A | <3s | ‚úÖ |
| Tasa de error | N/A | <5% | ‚úÖ |
| Satisfacci√≥n usuario | N/A | 90%+ | üéØ |

---

## üîê Consideraciones de Seguridad

1. ‚úÖ **API Key segura:** Nunca exponer en frontend, solo en Vercel env vars
2. ‚úÖ **Rate limiting:** Implementar en `/api/chat.js`
3. ‚úÖ **Input validation:** Validar mensaje antes de enviar a OpenAI
4. ‚úÖ **Error handling:** No exponer detalles internos al usuario
5. ‚úÖ **CORS:** Configurado en Vercel para producci√≥n

---

## üìö Documentaci√≥n Relacionada

- `docs/OPENAI_INTEGRATION.md` - Integraci√≥n OpenAI existente
- `apps/web/src/utils/invokeAgent.ts` - Funci√≥n unificada
- `packages/configs/agent-routing.json` - Configuraci√≥n de rutas
- `.env.example` - Variables de entorno

---

## ‚úÖ Checklist Final

- [ ] Import `invokeAgent` en Cockpit
- [ ] Reemplazar `handleSendMessage()` 
- [ ] Probar en desarrollo (backend Node.js)
- [ ] Verificar `OPENAI_API_KEY` en Vercel
- [ ] Deploy a producci√≥n
- [ ] Probar 5 casos de prueba
- [ ] Documentar en CHANGELOG
- [ ] Commit + push
- [ ] Celebrar üéâ

---

**¬øListo para implementar?** Empezamos con Fase 1. üöÄ
